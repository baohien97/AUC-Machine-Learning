{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Lab Assignment 2: Evaluate classifiers (10 points)\n",
    " \n",
    "In this assignment you will optimize and compare the perfomance of a parametric (logistic regression) and non-parametric (k-nearest neighbours) classifier on the MNIST dataset.\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning repository on Github ON TIME. We will check the last commit on the day of the deadline.  \n",
    "\n",
    "### Deadline Friday, November 17, 23:59.\n",
    "\n",
    "This notebook consists of three parts: design, implementation, results & analysis. \n",
    "We provide you with the design of the experiment and you have to implement it and analyse the results.\n",
    "\n",
    "### Criteria used for grading\n",
    "* Explain and analyse all results.\n",
    "* Make your notebook easy to read. When you are finished take your time to review it!\n",
    "* You do not want to repeat the same chunks of code multiply times. If your need to do so, write a function. \n",
    "* The implementation part of this assignment needs careful design before you start coding. You could start by writing pseudocode.\n",
    "* In this exercise the insights are important. Do not hide them somewhere in the comments in the implementation, but put them in the Analysis part\n",
    "* Take care that all the figures and tables are well labeled and numbered so that you can easily refer to them.\n",
    "* A plot should have a title and axes labels.\n",
    "* You may find that not everything is 100% specified in this assignment. That is correct! Like in real life you probably have to make some choices. Motivate your choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading points distribution\n",
    "\n",
    "* Implementation 5 points\n",
    "* Results and analysis 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have to keep the order of this design and are allowed to alter it if you are confident.\n",
    "* Import all necessary modules. Try to use as much of the available functions as possible. \n",
    "* Use the provided train and test set of MNIST dataset.\n",
    "* Pre-process data eg. normalize/standardize, reformat, etc.           \n",
    "  Do whatever you think is necessary and motivate your choices.\n",
    "* (1) Train logistic regression and k-nn using default settings.\n",
    "* Use 10-fold cross validation for each classifier to optimize the performance for one parameter: \n",
    "    * consult the documentation on how cross validation works in sklearn (important functions:             cross_val_score(), GridSearchCV()).\n",
    "    * Optimize k for k-nn,\n",
    "    * for logistic regression focus on the regularization parameter,\n",
    "* (2) Train logistic regression and k-nn using optimized parameters.\n",
    "* Show performance on the cross-validation set for (1) and (2) for both classifiers: \n",
    "    * report the average cross validation error rates (alternatively, the average accuracies - it's up to you) and standard deviation,\n",
    "    * plot the average cross valildation errors (or accuracies) for different values of the parameter that you tuned. \n",
    "* Compare performance on the test set for two classifiers:\n",
    "    * produce the classification report for both classifiers, consisting of precision, recall, f1-score. Explain and analyse the results.\n",
    "    * print confusion matrix for both classifiers and compare whether they missclassify the same  classes. Explain and analyse the results.\n",
    "* Discuss your results.\n",
    "* BONUS: only continue with this part if you are confident that your implemention is complete \n",
    "    * tune more parameters of logistic regression\n",
    "    * add additional classifiers (NN, Naive Bayes, decision tree), \n",
    "    * analyse additional dataset (ex. Iris dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all libraries and modules\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "from sklearn import datasets # to load the dataset\n",
    "from sklearn.model_selection import train_test_split #to split in train and test set\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV #BONUS\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix # for reporting\n",
    "from sklearn.preprocessing import StandardScaler # to normalize data (NN is very sensitive to this!)\n",
    "from sklearn.neighbors import KNeighborsClassifier # k nearest neighbour classifier\n",
    "from sklearn.neural_network import MLPClassifier # neural network classifier\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression classifier\n",
    "from sklearn.naive_bayes import GaussianNB # for BONUS\n",
    "from sklearn.tree import DecisionTreeClassifier # for BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset and split in train and test set.\n",
    "digits = datasets.load_digits()\n",
    "X_train_mnist = np.reshape(digits.images[:1500],(1500,64))\n",
    "X_test_mnist = np.reshape(digits.images[1500:],(297,64))\n",
    "y_train_mnist = digits.target[:1500]\n",
    "y_test_mnist = digits.target[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocess data (normalisation)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_mnist)\n",
    "scaler.fit(X_test_mnist)\n",
    "normalised_X_train = scaler.transform(X_train_mnist) # output scaled data in new variables\n",
    "normalised_X_test = scaler.transform(X_test_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train logistic regression and k-nn using default settings.\n",
    "log_regr = LogisticRegression()\n",
    "k_nn_classifier = KNeighborsClassifier()\n",
    "log_regr.fit(normalised_X_train, y_train_mnist)\n",
    "k_nn_classifier.fit(normalised_X_train, y_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_accuracy(moment, clf_name, clf, x_train, y_train):\n",
    "    cv_error = cross_val_score(clf, x_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "    print(moment + \" customisation, average accuracy of \" + clf_name + \" : \", cv_error.mean(), \" standard deviation: \", np.std(cv_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before customisation, average accuracy of LR :  0.941446300269  standard deviation:  0.0205180253743\n",
      "Before customisation, average accuracy of KNN :  0.948266646885  standard deviation:  0.0303457070823\n"
     ]
    }
   ],
   "source": [
    "#Use 10-fold cross validation for each classifier to optimize the performance for one parameter:\n",
    "#10-fold cross-validation, before tuning\n",
    "report_accuracy(\"Before\", \"LR\", log_regr, normalised_X_train, y_train_mnist)\n",
    "report_accuracy(\"Before\", \"KNN\", k_nn_classifier, normalised_X_train, y_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_single_param(clf, param_name, param_grid, x_train, y_train):\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    best_param = list(grid_search.best_params_.values())[0]\n",
    "    plt.plot(param_grid[param_name], grid_search.cv_results_['mean_test_score']) # plot accuracy scores against parameters\n",
    "    plt.xlabel(\"Parameters\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9h36lngAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea5786c588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimize k for k-nn with GridSearchCV\n",
    "k_param = {'n_neighbors': np.arange(1, 10)} # number of k neighbours from 1 to 10\n",
    "best_K = tune_single_param(KNeighborsClassifier(), 'n_neighbors', k_param, normalised_X_train, y_train_mnist)\n",
    "#print(best_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHItJREFUeJzt3X+UV3W97/HnixlgBgUGjBQZEG6hOceVmBNXb9k1rZNa\nJ46uSr2n28mTcblLE6t1yzzdc85dp7uW3VUZnTwhlTfPrRNZapmHjpXWke41ZZRBQEQQK0CQIYOB\nFBB83z/2HtjzZYb5DLDnO/Od12OtWfPdn/3Z33l/Buf7cu/P/qGIwMzMrC8jql2AmZkNDQ4MMzNL\n4sAwM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAwM7Mk9dUu4Hh6zWteE9OnT692GWZm\nQ8bjjz++PSImpfStqcCYPn06bW1t1S7DzGzIkPTb1L4+JGVmZkkcGGZmlsSBYWZmSRwYZmaWxIFh\nZmZJHBhmZpbEgWFmZkkcGAURwY/aN7Ni445ql2JmNujU1IV7x+Klffv59N0r+fGK56kbIW64aCbX\nvf111Nc5U83MwIEBwMYXX+Kj/9TGMy/s4pPvPJ1nO3Zz68+fYem6Dm69chZTJ46pdolmZlXnwAD+\n4aF1/Pb3L/Gta2bzttOzW6pceMZr+ewPV3HZgqV87vKzmDNrSpWrNDOrrlKPt0i6RNJaSesl3dTD\n+gmS7pX0pKTHJJ1Vsb5O0nJJ95dZ50v7DjC5qeFgWAD8+TlT+Mn8Czj9lLHMX9zOx7/XTueeV8os\nw8xsUCstMCTVAbcBlwItwNWSWiq63Qy0R8QbgQ8BCyrWzwfWlFVjl+ilferEMXxv7nnc+I6Z/Kh9\nM5ctWMrjv32x7HLMzAalMvcwZgPrI2JDROwDFgNzKvq0AA8BRMTTwHRJJwNIagbeDXyjxBr7VF83\nghvfcTrfn3c+AO9f+Ai3/uwZ9h94tZplmZkNuDIDYwqwsbC8KW8rWgFcASBpNnAa0Jyv+zLwKWBA\nPpnVx/pzT5vIkvkXMGfWFBY8uI4rF/2ajS++NBClmZkNCtU+Z/QWoElSO/AxYDlwQNJ7gG0R8Xhf\nbyBprqQ2SW0dHR1HV0Vvx6QqjGsYya1XzmLBVbN4ZusuLluwlB8u33x0P9PMbIgpMzA2A1MLy815\n20ER0RkR10TELLI5jEnABuAtwHsl/YbsUNZFkr7d0w+JiEUR0RoRrZMmJT00qkdSX/sYh8yZNYUl\n8y/gjFPGcuP32pm/eLknxM2s5pUZGMuAmZJmSBoFXAXcV+wgqSlfB3At8HAeIp+JiOaImJ5v91BE\nfLCsQiN1F6Ng6sQxLJ57Hp945+nc/+QWLluwlLbfeELczGpXaYEREfuB64EHyM50uisiVkuaJ2le\n3u1MYJWktWRnU80vq54y1NeN4IaLZ3LXfzkfCT5wuyfEzax2lXrhXkQsAZZUtC0svH4EOL2P9/gl\n8MsSyusm/YDU4c49bQJLbriAv/3RahY8uI6l6zpYcNU5vkLczGpKtSe9B4Xo/xGpw4xtGMmX8gnx\nddt2c+mCpdy7fNOxv7GZ2SDhwMj1Y877iObMyq4QP3PyWD7+vRWeEDezmuHA4PjsYRQ1TxjD4rnn\n88l8QvzSLy9lmSfEzWyIc2CUpG6E+NjFM/n+vPOpGyGuvP0RvvTTtZ4QN7Mhy4EBbN7xMjteKuew\n0ZumTeBfbngrl5/TzFceWs/7b3+E3/3eV4ib2dDjwABWbt7Jtl17S3v/sQ0j+eIHzuYfrj6H9dt2\nc9lXlnLPE5uI430szMysRA6MAfRnZ5/KT+ZfQMvkcXzirhXcsLidnS97QtzMhgY/QGmANU8Yw3fn\nnsfXfrmeW3++jgfXvMDUCWOY3NTA5PENTB7fyCnjGzi163tTA2NG+Z/JzKrPn0RVUDdCXH/RTC6Y\nOYm7n9jElp172LLzZVZt3sn23fsO6z+uoZ7J4xsPC5Wu15PHN3DCaP9Tmlm5/ClTRWdPbeLsqU3d\n2vbuP8ALO/eyZefLeZDsOfh66849yaFyyrjuAeNQMbNj5U+QQWZ0fR3TThrDtJN6v61IMVS2du7h\n+R172LrzZZ7vI1TGNtR3O9TVPVSyYHGomFlv/OkwBKWGyrbOvTy/o3uodO21rH6+k+27Dz8zrMdQ\nGd/QbW/FoWI2PPkvv0aNrq9j6sQxR7wBYmWobNm5hy070kKleKjr4Peuw2HjGznRoWJWc/xXPYz1\nJ1SKcyldobK1s/+hcnDPxaFiNuT4L9aOKCVU9u1/lRc6D5+g79pzSQ2VytOJHSpmg4v/Gu2Yjaof\nccyh8tSWTjp6uNp+7Oh6JufhcWpFqGSHwRwqZgPFf2k2II4mVLZWnFa8ph+h0n1uxaFidjz4r8gG\njf6ESnbmV8+hsn333sNuWT92dH0WJE2NTB7XcNg1KqeMb2Bsw8iSR2g2tDkwbEg51lDZehShUtxz\ncajYcObAsJrT31DpfjpxWqj0NEHvULFa58CwYSk1VLbt2nPwupSDpxPnwbJ26y46egiVE0fXH3bW\nV9dcStdV9Q4VG4ocGGa9GFU/guYJY2iekB4qW3e+nF9Vnx4qlRP0DhUbrBwYZscgJVReOZAf/tq5\nJ7/fVyFUOvewdmtHr6FyKFAOD5VTxjcwzqFiA8iBYVaykXVHFyrZYbCjC5VsbqXRoWLHVamBIekS\nYAFQB3wjIm6pWD8BuAN4HbAH+KuIWCWpAXgYGJ3X+IOI+NsyazWrptRQ2bZrb7cJ+mKoPPNCB9t2\n9R0q3S6CzENl7Oh6JJU8ShvqSgsMSXXAbcA7gU3AMkn3RcRThW43A+0RcbmkN+T9Lwb2AhdFxG5J\nI4FfSfpJRPy6rHrNBruRdSOY0tTIlKbGXvtUhkq2x/LywT2X3kLlhFF13eZPThnf2H2vpcmhYuXu\nYcwG1kfEBgBJi4E5QDEwWoBbACLiaUnTJZ0cES8Au/M+I/Oviv/EzaxSf0KlOJfS31A5ZVz3CXqH\nyvBQZmBMATYWljcB/76izwrgCmCppNnAaUAz8EK+h/I48Hrgtoh4tKcfImkuMBdg2rRpx3UAZrWo\nGCrnntZzn55CpXgY7EihcvBQ17juE/RdpxiPa3CoDFXVnvS+BVggqR1YCSwHDgBExAFglqQm4F5J\nZ0XEqso3iIhFwCKA1tZW74WYHQf9DZWDcylHEyr5mV8OlcGvzMDYDEwtLDfnbQdFRCdwDYCy/zqe\nAzZU9Nkh6RfAJcBhgWFm1ZF6+Ktj197uE/Q797C1M9tzWffCdrbt2sOrvYTK5OJcikOl6soMjGXA\nTEkzyILiKuA/FTvkew8vRcQ+4Frg4YjolDQJeCUPi0ayifPPl1irmZVgZN0ITm1q5NR+hEp22/tD\nobJ0Xc+hMmZUXcVDuvLJ+sKNJR0qx1dpgRER+yVdDzxAdlrtHRGxWtK8fP1C4EzgTkkBrAY+km8+\nOW+vA0YAd0XE/WXVambVkxIq+7vO/irc76srVLbsTAuVbO/EoXIsSp3DiIglwJKKtoWF148Ap/ew\n3ZPAOWXWZmZDR323UJnQY5+eQqU4p/KrI4RKt5tJFk8rbmpg8rhGxjU6VKD6k95mZsdFaqh07N7b\n7X5fxVD5v+u380Jn36HS/VYtwydUHBhmNmzU143IP+iPfPirp1Dpul6lr1DpPq/S/bTioR4qDgwz\ns4L+hMqhM7/SQqVxZF2PT3s8dDhscIeKA8PMrJ+6hUov1wv3FCrFeZUjhkp+qOuUcY35A7oGR6g4\nMMzMStDfUMnO/OoeKv/v2bRQmTZxDPPfMbP8MZX+E8zMrEeph7+279536H5fXaHSmT0F8pFnt7Ps\nNyMcGGZmw1193YiDz5HvzauVuyAlGTEgP8XMzEozYsTAzGc4MMzMLIkDw8zMkjgwzMwsiQPDzMyS\nODDMzCyJA8PMzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMySODDMzCyJA8PMzJI4MMzMLIkDw8zMkjgw\nzMwsSamBIekSSWslrZd0Uw/rJ0i6V9KTkh6TdFbePlXSLyQ9JWm1pPll1mlmZn0rLTAk1QG3AZcC\nLcDVkloqut0MtEfEG4EPAQvy9v3AJyOiBTgPuK6Hbc3MbACVuYcxG1gfERsiYh+wGJhT0acFeAgg\nIp4Gpks6OSK2RMQTefsuYA0wpcRazcysD2UGxhRgY2F5E4d/6K8ArgCQNBs4DWgudpA0HTgHeLSk\nOs3MLEG1J71vAZoktQMfA5YDB7pWSjoRuBu4MSI6e3oDSXMltUlq6+joGIiazcyGpfoS33szMLWw\n3Jy3HZSHwDUAkgQ8B2zIl0eShcV3IuKe3n5IRCwCFgG0trbGcazfzMwKytzDWAbMlDRD0ijgKuC+\nYgdJTfk6gGuBhyOiMw+PbwJrIuJLJdZoZmaJStvDiIj9kq4HHgDqgDsiYrWkefn6hcCZwJ2SAlgN\nfCTf/C3AfwZW5oerAG6OiCVl1WtmZkdW5iEp8g/4JRVtCwuvHwFO72G7XwEqszYzM+ufak96m5nZ\nEOHAMDOzJA4MMzNL4sAwM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJEmBIen9ksbmrz8r6R5Jbyq3\nNDMzG0xS9zD+e0TskvRW4B1k93n6WnllmZnZYJMaGF23HH83sCgi/gUYdYT+ZmZWY1IDY7Ok24Er\ngSWSRvdjWzMzqwGpH/ofILvr7LsiYgcwEfhvpVVlZmaDTlJgRMRLwDbgrXnTfmBdWUWZmdngk3qW\n1N8CnwY+kzeNBL5dVlFmZjb4pB6Suhx4L/BHgIh4HhhbVlFmZjb4pAbGvogIIAAknVBeSWZmNhil\nBsZd+VlSTZI+Cvwc+Hp5ZZmZ2WCT9IjWiPiCpHcCncAZwN9ExM9KrczMzAaVPgNDUh3w84h4O+CQ\nMDMbpvo8JBURB4BXJY0fgHrMzGyQSjokBewGVkr6GfmZUgARcUMpVZmZ2aCTGhj35F9mZjZMpU56\n3ylpFHB63rQ2Il4prywzMxtsUq/0vpDsViC3Af8IPCPpbQnbXSJpraT1km7qYf0ESfdKelLSY5LO\nKqy7Q9I2SauSR2NmZqVJvQ7ji8CfRsR/jIi3Ae8Cbj3SBvnZVbcBlwItwNWSWiq63Qy0R8QbgQ8B\nCwrrvgVcklifmZmVLDUwRkbE2q6FiHiG7H5SRzIbWB8RGyJiH7AYmFPRpwV4KH/Pp4Hpkk7Olx8G\nXkysz8zMSpYaGG2SviHpwvzr60BbH9tMATYWljflbUUrgCsAJM0GTgOaE2syM7MBlBoY/xV4Crgh\n/3oqbztWt5DdbqQd+BiwnENP90siaa6kNkltHR0dx6EkMzPrSepptfXAgoj4EhycnxjdxzabgamF\n5ea87aCI6ASuyd9TwHPAhsSaut5jEbAIoLW1NfqzrZmZpUvdw3gQaCwsN5LdgPBIlgEzJc3IT8m9\nCriv2EFSU74O4Frg4TxEzMxskEkNjIaI2N21kL8ec6QNImI/cD3Zo13XAHdFxGpJ8yTNy7udCayS\ntJbsbKr5XdtL+i7wCHCGpE2SPpI6KDMzO/5SD0n9UdKbIuIJAEmtwMt9bRQRS4AlFW0LC68f4dDF\ngJXbXp1Ym5mZDYDUwLgR+L6k5/PlycCV5ZRkZmaD0REPSUl6s6RTImIZ8Abge8ArwL+STVCbmdkw\n0dccxu3Avvz1+WRXZt8G/IH8zCQzMxse+jokVRcRXVdbXwksioi7gbvzayfMzGyY6GsPo05SV6hc\nTH4bj1zq/IeZmdWAvj70vwv8m6TtZGdFLQWQ9HpgZ8m1mZnZIHLEwIiI/ynpQbKzon4aEV1XUo8g\nu5WHmZkNE30eVoqIX/fQ9kw55ZiZ2WCVeqW3mZkNcw4MMzNL4sAwM7MkDgwzM0viwDAzsyQODDMz\nS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAwM7MkpQaGpEsk\nrZW0XtJNPayfIOleSU9KekzSWanbmpnZwCotMCTVAbcBlwItwNWSWiq63Qy0R8QbgQ8BC/qxrZmZ\nDaAy9zBmA+sjYkNE7AMWA3Mq+rQADwFExNPAdEknJ25rZmYDqMzAmAJsLCxvytuKVgBXAEiaDZwG\nNCdua2ZmA6jak963AE2S2oGPAcuBA/15A0lzJbVJauvo6CijRjMzA+pLfO/NwNTCcnPedlBEdALX\nAEgS8BywAWjsa9vCeywCFgG0trbGcardzMwqlLmHsQyYKWmGpFHAVcB9xQ6SmvJ1ANcCD+ch0ue2\nZmY2sErbw4iI/ZKuBx4A6oA7ImK1pHn5+oXAmcCdkgJYDXzkSNuWVauZmfWtzENSRMQSYElF28LC\n60eA01O3NTOz6qn2pLeZmQ0RDgwzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0vi\nwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAw\nM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNLUmpgSLpE0lpJ6yXd1MP68ZJ+LGmFpNWSrims\nmy9pVd5+Y5l1mplZ30oLDEl1wG3ApUALcLWklopu1wFPRcTZwIXAFyWNknQW8FFgNnA28B5Jry+r\nVjMz61uZexizgfURsSEi9gGLgTkVfQIYK0nAicCLwH7gTODRiHgpIvYD/wZcUWKtZmbWhzIDYwqw\nsbC8KW8r+ipZODwPrATmR8SrwCrgAkknSRoDXAZMLbFWMzPrQ32Vf/67gHbgIuB1wM8kLY2INZI+\nD/wU+GPe50BPbyBpLjAXYNq0aQNStJnZcFTmHsZmuu8VNOdtRdcA90RmPfAc8AaAiPhmRJwbEW8D\n/gA809MPiYhFEdEaEa2TJk067oMwM7NMmYGxDJgpaYakUcBVwH0VfX4HXAwg6WTgDGBDvvza/Ps0\nsvmLfy6xVjMz60Nph6QiYr+k64EHgDrgjohYLWlevn4h8PfAtyStBAR8OiK2529xt6STgFeA6yJi\nR1m1mplZ30qdw4iIJcCSiraFhdfPA3/ay7YXlFmbmZn1j6/0NjOzJA4MMzNL4sAwM7MkDgwzM0vi\nwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAw\nM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0viwDAzsyT11S5gMPi7P2vh\nzTMmVrsMM7NBrdQ9DEmXSForab2km3pYP17SjyWtkLRa0jWFdR/P21ZJ+q6khrLq/PBbZvAnp44v\n6+3NzGpCaYEhqQ64DbgUaAGultRS0e064KmIOBu4EPiipFGSpgA3AK0RcRZQB1xVVq1mZta3Mvcw\nZgPrI2JDROwDFgNzKvoEMFaSgBOBF4H9+bp6oFFSPTAGeL7EWs3MrA9lBsYUYGNheVPeVvRV4Eyy\nMFgJzI+IVyNiM/AF4HfAFmBnRPy0px8iaa6kNkltHR0dx3sMZmaWq/ZZUu8C2oFTgVnAVyWNkzSB\nbG9kRr7uBEkf7OkNImJRRLRGROukSZMGqm4zs2GnzMDYDEwtLDfnbUXXAPdEZj3wHPAG4B3AcxHR\nERGvAPcA/6HEWs3MrA9lBsYyYKakGZJGkU1a31fR53fAxQCSTgbOADbk7edJGpPPb1wMrCmxVjMz\n60Np12FExH5J1wMPkJ3ldEdErJY0L1+/EPh74FuSVgICPh0R24Htkn4APEE2Cb4cWFRWrWZm1jdF\nRLVrOG5aW1ujra2t2mWYmQ0Zkh6PiNakvrUUGJI6gN8e5eavAbYfx3KGAo+59g238YLH3F+nRUTS\nGUM1FRjHQlJbasrWCo+59g238YLHXKZqn1ZrZmZDhAPDzMySODAOGY5nYXnMtW+4jRc85tJ4DsPM\nzJJ4D8PMzJIM+8Do65kdQ5WkqZJ+Iemp/Lki8/P2iZJ+Jmld/n1CYZvP5L+HtZLeVb3qj56kOknL\nJd2fL9f0eAEkNUn6gaSnJa2RdH4tj7unZ+XU4ngl3SFpm6RVhbZ+j1PSuZJW5uu+kt894+hExLD9\nIrsC/Vng3wGjgBVAS7XrOk5jmwy8KX89FniG7Lkk/wu4KW+/Cfh8/rolH/9osps+PgvUVXscRzHu\nTwD/DNyfL9f0ePOx3Alcm78eBTTV6rjJ7nj9HNCYL98FfLgWxwu8DXgTsKrQ1u9xAo8B55HdTeMn\nwKVHW9Nw38NIeWbHkBQRWyLiifz1LrJ7cU0hG9+debc7gT/PX88BFkfE3oh4DlhP9vsZMiQ1A+8G\nvlFortnxQvbUSrIPlm8CRMS+iNhBbY+7p2fl1Nx4I+JhsmcEFfVrnJImA+Mi4teRpcc/Fbbpt+Ee\nGCnP7BjyJE0HzgEeBU6OiC35qq3AyfnrWvhdfBn4FPBqoa2WxwvZ/012AP87PxT3DUknUKPjjt6f\nlVOT4+1Bf8c5JX9d2X5Uhntg1DxJJwJ3AzdGRGdxXf5/HDVxmpyk9wDbIuLx3vrU0ngL6skOW3wt\nIs4B/kh2qOKgWhp3yrNyamm8R1KNcQ73wEh5ZseQJWkkWVh8JyLuyZtfyHdTyb9vy9uH+u/iLcB7\nJf2G7NDiRZK+Te2Ot8smYFNEPJov/4AsQGp13L09K6dWx1upv+PcnL+ubD8qwz0wUp7ZMSTlZ0J8\nE1gTEV8qrLoP+Mv89V8CPyq0XyVptKQZwEyyybIhISI+ExHNETGd7N/xoYj4IDU63i4RsRXYKOmM\nvOli4Clqd9y9PSunVsdbqV/jzA9fdUo6L/99faiwTf9V+0yAan8Bl5GdQfQs8NfVruc4juutZLur\nT5I9Brc9H+tJwIPAOuDnwMTCNn+d/x7WcgxnUlT7C7iQQ2dJDYfxzgLa8n/rHwITanncwP8AngZW\nAf+H7Mygmhsv8F2yeZpXyPYkP3I04wRa89/Vs8BXyS/YPpovX+ltZmZJhvshKTMzS+TAMDOzJA4M\nMzNL4sAwM7MkDgwzM0viwLBhSdIBSe35HU+/L2lMtWsCkHRztWsw641Pq7VhSdLuiDgxf/0d4PHo\nfoHjkbati4gDZdfVj21Kq8esyHsYZrAUeD2ApB9Kejx/3sLcrg6Sdkv6oqQVwPmS/kbSsnwPZVHX\nMwYk/VLSrZLa8mdTvFnSPfnzCz5XeL8PSnos38u5XdlzPG4huwtrex5iPfbrpZ5blD375ElJXxi4\nX50NJw4MG9byW2RfCqzMm/4qIs4luzr2Bkkn5e0nAI9GxNkR8SvgqxHx5og4C2gE3lN4230R0Qos\nJLsNw3XAWcCHJZ0k6UzgSuAtETELOAD8RUTcBLwcEbMi4i9661dZD9mtMS4H/iQi3gh8DrMS1Fe7\nALMqaZTUnr9eSv48CbKQuDx/PZXsnjy/J/uwvruw/dslfYrseQwTgdXAj/N1XfcjWwmsjvx21JI2\n5O/5VuBcYFm+Y9LIoZvIFV18hH7FenYCe4BvKnvS4P3JvwWzfnBg2HD1cv5/7QdJupDsbqjnR8RL\nkn4JNOSr93TNE0hqAP4RaI2IjZL+rtAPYG/+/dXC667lerInn90ZEZ/po8Yj9TtYT0TslzSbLGDe\nB1wPXNTHe5v1mw9JmR0yHvhDHhZvIHusZU+6wmF7/ryR9/Xz5zwIvE/Sa+Hgc5pPy9e9kt+Wvq9+\nB+U1jI+IJcDHgbP7WY9ZEu9hmB3yr8A8SWvI7vj56546RcQOSV8nuwPoVrLb5CeLiKckfRb4qaQR\nZHcjvQ74LbAIeFLSE/k8Rm/9isYCP8r3fET2XHOz486n1ZqZWRIfkjIzsyQODDMzS+LAMDOzJA4M\nMzNL4sAwM7MkDgwzM0viwDAzsyQODDMzS/L/AV7ZMGDnW+DYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea589a0b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tune lambda with GridSearchCV\n",
    "lambda_param = {'C': [0.1, 0.01, 0.001, 1, 10, 100, 1000]} # regularisation params to be considered\n",
    "best_C = tune_single_param(LogisticRegression(), 'C', lambda_param, normalised_X_train, y_train_mnist)\n",
    "#print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression and k-nn using optimized parameters.\n",
    "log_regr_customised = LogisticRegression(C=best_C)\n",
    "knn_customised = KNeighborsClassifier(n_neighbors=best_K)\n",
    "log_regr_customised.fit(normalised_X_train, y_train_mnist)\n",
    "knn_customised.fit(normalised_X_train, y_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After customisation, average accuracy of LR :  0.942180286678  standard deviation:  0.0191189588715\n",
      "After customisation, average accuracy of KNN :  0.959534823035  standard deviation:  0.0271404395113\n"
     ]
    }
   ],
   "source": [
    "#Show performance on the cross-validation set for (1) and (2) for both classifiers: \n",
    "#    * report the average cross validation error rates (alternatively, the average accuracies - it's up to you) \n",
    "#and standard deviation, after tuning\n",
    "report_accuracy(\"After\", \"LR\", log_regr_customised, normalised_X_train, y_train_mnist)\n",
    "report_accuracy(\"After\", \"KNN\", knn_customised, normalised_X_train, y_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression's accuracy:  0.878787878788\n",
      "Report for Logistic Regression :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.93      0.94        27\n",
      "          1       0.79      0.84      0.81        31\n",
      "          2       1.00      1.00      1.00        27\n",
      "          3       0.86      0.63      0.73        30\n",
      "          4       0.91      0.91      0.91        33\n",
      "          5       0.91      0.97      0.94        30\n",
      "          6       0.97      0.97      0.97        30\n",
      "          7       0.90      0.87      0.88        30\n",
      "          8       0.68      0.89      0.77        28\n",
      "          9       0.89      0.81      0.85        31\n",
      "\n",
      "avg / total       0.89      0.88      0.88       297\n",
      "\n",
      "KNN's accuracy:  0.929292929293\n",
      "Report for KNN :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        27\n",
      "          1       0.89      1.00      0.94        31\n",
      "          2       1.00      0.89      0.94        27\n",
      "          3       0.93      0.90      0.92        30\n",
      "          4       0.97      0.88      0.92        33\n",
      "          5       0.90      0.93      0.92        30\n",
      "          6       0.88      1.00      0.94        30\n",
      "          7       0.91      1.00      0.95        30\n",
      "          8       1.00      0.82      0.90        28\n",
      "          9       0.87      0.87      0.87        31\n",
      "\n",
      "avg / total       0.93      0.93      0.93       297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#* Compare performance on the test set for two classifiers:\n",
    "#    * produce the classification report for both classifiers, consisting of precision, recall, f1-score. \n",
    "def produce_report(clf_name, clf, x_test, y_test):\n",
    "    preds = clf.predict(x_test)\n",
    "    print(clf_name + \"'s accuracy: \", accuracy_score(y_test, preds))\n",
    "    print(\"Report for \" + clf_name + \" :\\n\", classification_report(y_test, preds))\n",
    "    return preds\n",
    "log_predictions = produce_report(\"Logistic Regression\", log_regr_customised, normalised_X_test, y_test_mnist)\n",
    "knn_predictions = produce_report(\"KNN\", knn_customised, normalised_X_test, y_test_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Explain and analyse Classification Reports\n",
    "In the two classification reports above, what can be seen is the clearly better performance of the K Nearest Neighbour (KNN) classifier on all scoring categories (precision, recall and f1-score), and hence its accuracy score is also significantly higher (0.93 compared to Logistic Regression's (LR) 0.88). It's worth noting that the number of k-nearest-neighbours that was classified as best by the `GridSearchCV` module in the cross-validation phase was 1, this means that there is a clear separation of labels between the instances of the training set `X_train_mnist` (or `normalised_X_train`).\n",
    "\n",
    "In the `precision` score ($tp/P$), while LR performed rather poorly in class 8 (0.68), KNN scored perfectly. The same thing can be seen with the `recall` score ($tp/(tp+fn)$) in class 3, where LR scored 0.63 and KNN scored 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Logistic Regression: \n",
      " [[25  0  0  0  1  0  1  0  0  0]\n",
      " [ 0 26  0  2  0  0  0  0  3  0]\n",
      " [ 0  0 27  0  0  0  0  0  0  0]\n",
      " [ 0  1  0 19  0  3  0  2  5  0]\n",
      " [ 0  0  0  0 30  0  0  0  0  3]\n",
      " [ 0  1  0  0  0 29  0  0  0  0]\n",
      " [ 0  1  0  0  0  0 29  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 26  3  0]\n",
      " [ 0  2  0  0  1  0  0  0 25  0]\n",
      " [ 1  2  0  1  0  0  0  1  1 25]] \n",
      "\n",
      "Confusion matrix for K Nearest Neighbours:\n",
      " [[27  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 31  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 24  0  0  0  3  0  0  0]\n",
      " [ 0  0  0 27  0  1  1  1  0  0]\n",
      " [ 0  0  0  0 29  0  0  1  0  3]\n",
      " [ 0  1  0  0  1 28  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 30  0  0]\n",
      " [ 0  3  0  1  0  0  0  0 23  1]\n",
      " [ 0  0  0  1  0  2  0  1  0 27]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix for both classifiers and compare whether they missclassify the same  classes. \n",
    "log_cm = confusion_matrix(y_test_mnist, log_predictions)\n",
    "knn_cm = confusion_matrix(y_test_mnist, knn_predictions)\n",
    "print(\"Confusion matrix for Logistic Regression: \\n\", log_cm, \"\\n\")\n",
    "print(\"Confusion matrix for K Nearest Neighbours:\\n\", knn_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain and analyse Confusion Matrices\n",
    "The two confusion matrices ($m$ x $n$) above present the performances of two classifiers on the test set (`normalised_X_test`) that contains 10 classes (from 0 to 9). Their diagonals reflect the number of instances that were predicted correctly for each class. While for each entry $E$ that is not on the diagonal, $E_{i,j}$ is the number of instances that are supposed to be in class $i$ but were misclassified as class $j$. Looking at these 2 matrices, one can see that both classifiers misclassified class 3, 4, 5, 8 and 9. For example, in class 3, LR only correctly classified 19 instances, and 1 instance was misclassified as class 1, 3 as class 5, 2 as class 7 and 5 as class 8; hence it produced a recall score of 0.63."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Discuss your results\n",
    "The implementation of LR and KNN in this notebook indicates the following points:\n",
    "\n",
    " - Using cross validation to tune the parameters, specifically, k and lambda, did produce an improvement in the accuracy score of both classifiers. The best k was found to be 1, while the best regularisation parameter lambda was 10. The `GridSearchCV` was of great help in finding these, due to its iterating over each value of the parameters and calculating the cross validation score of each. In the Bonus this module is also further exploited, however due to the heaviness of this notebook it `GridSearchCV` extremely long to produce output via its property `best_params_` when dealing with more than 1 parameter type, and hence I had to use it separately.\n",
    " - The reports and confusion matrices showed a better performance of the KNN classifier in the test set (normalised), with an average accuracy score of 0.93 in all scoring cateogories. Meanwhile, LR observed some low scores in class 3 for `recall` and 8 for `precision`. This indicates the efficiency of the KNN classifier in dealing with this particular dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load iris dataset, with 4 features, tried doing it with 2 features and the scores were really low for all classifiers\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:, : 4] \n",
    "labels = iris.target\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "features = scaler.transform(features)\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(features, labels)\n",
    "X_train_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cells below try to tune the two parameters `max_iter` and `penalty` of the `LogisticRegression` classifier. The two parameters could have been put into one dictionary called `param_grid` but doing that made my notebook extremely heavy and nearly crashed my whole system. So yes there's a redundancy of code but the notebook doesn't allow me to do otherwise :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_iter': 100}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune more parameters of logistic regression\n",
    "iter_grid = {'max_iter': [100, 200, 300, 400, 500]}\n",
    "gs = GridSearchCV(LogisticRegression(C=10), param_grid=iter_grid, cv=10)\n",
    "gs.fit(normalised_X_train, y_train_mnist)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty_grid = {'penalty': ['l1', 'l2']}\n",
    "gs = GridSearchCV(LogisticRegression(C=10), param_grid=penalty_grid, cv=10)\n",
    "gs.fit(normalised_X_train, y_train_mnist)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_preds(clf, set_name, x_train, x_test, y_train, y_test):\n",
    "    preds = clf.fit(x_train, y_train).predict(x_test)\n",
    "    print(\"accuracy score for \" + set_name +\" set: \", accuracy_score(y_test, preds))\n",
    "    print(\"classification report for \" + set_name + \" set: \\n\", classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for mnist set:  0.794612794613\n",
      "classification report for mnist set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        27\n",
      "          1       0.68      0.87      0.76        31\n",
      "          2       0.95      0.70      0.81        27\n",
      "          3       1.00      0.47      0.64        30\n",
      "          4       0.96      0.82      0.89        33\n",
      "          5       0.78      0.97      0.87        30\n",
      "          6       0.97      0.97      0.97        30\n",
      "          7       0.60      0.87      0.71        30\n",
      "          8       0.53      0.68      0.59        28\n",
      "          9       0.91      0.65      0.75        31\n",
      "\n",
      "avg / total       0.84      0.79      0.79       297\n",
      "\n",
      "accuracy score for iris set:  0.973684210526\n",
      "classification report for iris set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       0.93      1.00      0.96        13\n",
      "\n",
      "avg / total       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB\n",
    "bayes_classifier = GaussianNB()\n",
    "classifier_preds(bayes_classifier, \"mnist\", normalised_X_train, normalised_X_test, y_train_mnist, y_test_mnist)\n",
    "classifier_preds(bayes_classifier, \"iris\", X_train_iris, X_test_iris, y_train_iris, y_test_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for mnist set:  0.784511784512\n",
      "classification report for mnist set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.96      0.95        27\n",
      "          1       0.74      0.65      0.69        31\n",
      "          2       0.81      0.78      0.79        27\n",
      "          3       0.70      0.63      0.67        30\n",
      "          4       0.79      0.91      0.85        33\n",
      "          5       0.81      0.87      0.84        30\n",
      "          6       0.96      0.83      0.89        30\n",
      "          7       0.70      0.87      0.78        30\n",
      "          8       0.58      0.54      0.56        28\n",
      "          9       0.83      0.81      0.82        31\n",
      "\n",
      "avg / total       0.79      0.78      0.78       297\n",
      "\n",
      "accuracy score for iris set:  0.947368421053\n",
      "classification report for iris set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       0.86      1.00      0.92        12\n",
      "          2       1.00      0.85      0.92        13\n",
      "\n",
      "avg / total       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "dec_tree = DecisionTreeClassifier(random_state=1)\n",
    "classifier_preds(dec_tree, \"mnist\", normalised_X_train, normalised_X_test, y_train_mnist, y_test_mnist)\n",
    "classifier_preds(dec_tree, \"iris\", X_train_iris, X_test_iris, y_train_iris, y_test_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layer_sizes': (200,)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN, tuning number of layers\n",
    "layer_grid = {'hidden_layer_sizes': [(100,), (200,), (300,), (400,)]}\n",
    "#, 'activation': ['identity', 'logistic', 'tanh', 'relu'], 'alpha': [0.1, 0.01, 0.001, 0.0001, 1], 'max_iter': [100, 200, 300, 400, 500]}\n",
    "gs_nn = GridSearchCV(MLPClassifier(), param_grid=layer_grid, cv=10)\n",
    "gs_nn.fit(normalised_X_train, y_train_mnist).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for mnist set:  0.912457912458\n",
      "classification report for mnist set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.93      0.94        27\n",
      "          1       0.94      1.00      0.97        31\n",
      "          2       1.00      1.00      1.00        27\n",
      "          3       0.95      0.60      0.73        30\n",
      "          4       0.97      0.91      0.94        33\n",
      "          5       0.88      1.00      0.94        30\n",
      "          6       0.97      0.97      0.97        30\n",
      "          7       0.88      0.93      0.90        30\n",
      "          8       0.74      0.93      0.83        28\n",
      "          9       0.90      0.87      0.89        31\n",
      "\n",
      "avg / total       0.92      0.91      0.91       297\n",
      "\n",
      "accuracy score for iris set:  1.0\n",
      "classification report for iris set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        12\n",
      "          2       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(200,), max_iter=500)\n",
    "classifier_preds(nn_classifier, \"mnist\", normalised_X_train, normalised_X_test, y_train_mnist, y_test_mnist)\n",
    "classifier_preds(nn_classifier, \"iris\", X_train_iris, X_test_iris, y_train_iris, y_test_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some comments.\n",
    "\n",
    "What can be seen from the examples above is that the classifiers obtained better performance in the iris dataset than the mnist dataset, especially in the case of the Decision Tree and the Naive Bayes. It is worth noting that the mnist dataset contained 1500 training examples and 64 features, compared to 112 examples and 4 features of the iris set. Therefore, what may have happened in the Decision Tree classification was overfitting: with such a large number of features, the number of splits became larger and larger and this may have resulted in situations where singleton = pure $\\Rightarrow$ not able to deal with the test set. Meanwhile, with Naive Bayes, which constitutes a simple linear hypothesis function, the problem was likely underfitting. This means that the large number of data was the cause for the classifier not fitting the data well and hence did not perform well on the test set either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
