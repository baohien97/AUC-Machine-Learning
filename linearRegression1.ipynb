{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost(theta0, theta1, x, y):\n",
    "    #your code here\n",
    "    return result\n",
    "\n",
    "print(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    #your code here\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
